{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook review_subset_generator_20180305.ipynb to script\n",
      "[NbConvertApp] Writing 16568 bytes to review_subset_generator_20180305.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to script review_subset_generator_20180305.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "import re\n",
    "\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "\n",
    "import spacy\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stars_filepath = 'review_stars_rest_subset.txt'\n",
    "review_txt_filepath = 'review_text_rest_subset.txt'\n",
    "business_filepath = 'review_business_rest_subset.txt'\n",
    "user_filepath = 'review_user_rest_subset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(filepath,review_number):\n",
    "    \"\"\"\n",
    "    retrieve a particular review index\n",
    "    from the reviews file and return it\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(it.islice(line_review(filepath),\n",
    "                          review_number, review_number+1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(stars_filepath) as f:\n",
    "    stars = f.readlines()\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "stars = [x.strip() for x in stars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(review_txt_filepath) as f:\n",
    "    texts = f.readlines()\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "texts = [x.strip() for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(business_filepath) as f:\n",
    "    business = f.readlines()\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "business = [x.strip() for x in business]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(user_filepath) as f:\n",
    "    user = f.readlines()\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "user = [x.strip() for x in user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570963 1570963 1570963 1570963\n"
     ]
    }
   ],
   "source": [
    "#test lengths, all should be the same\n",
    "print(len(stars), len(texts), len(business), len(user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52810\n"
     ]
    }
   ],
   "source": [
    "bus_set = frozenset(business)\n",
    "print(len(bus_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169373\n"
     ]
    }
   ],
   "source": [
    "user_set = frozenset(user)\n",
    "print(len(user_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'hjk3ox7w1akbEuOgTJ03Bw', 'RtUvSWO_UZ8V3Wpj0n077w', 'zgQHtqX0gqMw1nlBZl2VnQ', 'zxJlg4XCHNoFy78WZPv89w', 'oWTn2IzrprsRkPfULtjZtQ', 'N93EYZy9R0sdlEvubu94ig', 'Eox_Qq74oaFZ-YjthpHhBw', '0W4lkclzZThpx3V65bVgig', 'I8rveLd-dl81u6c8YqAxmw', 'a9aW5e731lplWGHUZ02-zQ', 'Aov96CM4FZAXeZvKtsStdA', 'XWTPNfskXoUL-Lf32wSk0Q', 'z8oIoCT1cXz7gZP5GeU5OA', 'Xy74meQwdTnloAAyRC-4cg', 'PFPUMF38-lraKzLcTiz5gQ', 'ZnxudK5ExgpfXs4bicS4IA', '4_GIJk0tX3k0x0FcUv4sNA', '28adZ4lsuUeVB2aWzohK9g', '5r6-G9C4YLbC7Ziz57l3rQ'})\n"
     ]
    }
   ],
   "source": [
    "test_set = frozenset(business[1:20])\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make collased business text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_bus_filepath = \"collapsed_business_rest_subset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_test_filepath = \"collapsed_business_rest_subset_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#function to loop through all business ids, collapse all reviews for that business into one string\n",
    "if 1 == 0:\n",
    "    with codecs.open(all_bus_filepath, 'w', encoding='utf_8') as f:\n",
    "        for x in bus_set:\n",
    "            review_index = [i for i,j in enumerate(business) if j == x]\n",
    "            review_sub = [texts[ind] for ind in review_index]\n",
    "            review_out = ''.join(map(str, review_sub))\n",
    "            f.write(review_out + '\\n')\n",
    "            #print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(all_bus_filepath) as f:\n",
    "    all_bus = f.readlines()\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "all_bus = [x.strip() for x in all_bus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52810"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_bus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This place is all around mediocre.  It feels quite dated and the menu is quite scattered. The food was okay, but the presentation could be better.  My biggest complaint is the service which could be a bit nicer. Over the few times I\\'ve been here, I\\'ve noticed the staff can be quite impatient with you.  I wouldn\\'t go here again by choice, but if you need to eat in this area and are out of choices, it\\'s not the end of the world to have a meal here.I came back here a second time, I was working in the area and wanted quick cheap food. I tried the souvlaki this time and what a difference. The burger I took the first time was really bad but the chicken souvlaki tasted better then some other Greek restaurant I have been to. This time their fries were all right, I\\'m glad I happen to give this place a second chance, just stay away from the burger. Give this place a try if you don\\'t have a good budget.A very traditional American diner, and everything else you wouldn\\'t find in the US.  My colleagues and I meet twice a week for breakfast. The wait staff is friendly and makes sure your white coffee mug is always full of coffee.  If you love pancakes definitely order them.  Other favorites are french toast, a breakfast sandwich with fried egg, lettuce, bacon and processed cheese.  We ask for it on a dry wheat toast and no mayo.Beacoup de place à l\\'interieur et meme en arriere; bouffe rapide, frites, pizza, burger, pasta, salade grecque, etc.Filthy ugly place. Terrible service. Bad quality food, badly prepared.   Rips in upholstery everywhere. Carpets worn through and absolutely grimy. Most hard surfaces are sticky with dirt. My burger (pretty much the most standard easy item that can be ordered in a diner) was served very well done. I had ordered it medium rare. However, considering the quality of the meat I think perhaps they burnt it for safety.   The grossest part of the experience was watching the waitress openly stuffing her mouth with leftover food from plates right at the tables, just as soon as customers walked out. I witnessed this several times. Once in fact when I signaled for the check and her mouth was so full she could not respond. So she pointed at her mouth and gave me a thumbs up, I guess implying \"My mouth is full of table scraps so I cannot speak at this moment. However, I recognize that you\\'ve stopped consuming your disgusting burger and are requesting the check. This actually works out well because I noticed you only managed to get halfway through that burger and I\\'m still hungry. So I\\'ll hurry and get your bill if you could go ahead and get going just as quickly as you can. Thanks! Hope to see you again soon!\"Very affordable diner. Great offering and fresh. Waitress was fantastic and extremely welcoming. Offered insight on locale attractions this place is frequented by locals.The sausage is too salty. The cheese in smoked meat isnt melted. The service was pretty good.Just stopped in here for a quick lunch. the menu seemed to have lots of choice and the prices were okay.  On the corner of Notre-Dame Ouest and McGill, this little \\'deli-grill\\', as they call themselves, is packed only with booths.  The service was very quick! I was pleasantly surprised. I guess they really specialize in the lunch crowd, because i took my sweet time eating and sipping my tea, and i was still in and out of there in 40 mins. tops! I got there just before noon to find maybe 5 or 6 other booths occupied. Before i knew it every booth in the place was taken.  I started with a French Onion soup. Really, i could eat french onion soup every damn day, i love it so much. This soup was good. Nice and cheesy. But nothing to rave about. Just good soup. Next a had a small poutine. Sad to say, i was disappointed.  I think the gravy was chicken gravy, it had that slightly runny and peppered taste to it. Not as good as delicious beef gravy. Here\\'s the kicker, the cheese was just regular old mozzarella! *Gasp!* No cheese curds?! Montrealers should know better. I want my squeeky cheese poutine! Ah well, i ate maybe half of it.  I would come back again to try something else. They have a huge selection of hot or cold sandwiches, as well as greek, italian and steaks and burgers too.Used to go there all the time when I worked in the area, it was a decent joint for what it served but now years later, it was horrible. Burger felt like it was twice cooked and basically a hockey puck, fries tasted like canola oil.  Anywho I dot recommend it to anyone but the starving and I ain\\'t never going back.Une devanture peut attirante vue de l\\'extérieur mais accueillant à l\\'intérieur. Nourriture à mon gout classique pour un brunch. Assiette très copieuse et service attentif.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bus[749]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(all_test_filepath) as f:\n",
    "    test_bus = f.readlines()\n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "test_bus = [x.strip() for x in test_bus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_bus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make collapsed user text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_user_filepath = \"collapsed_user_rest_subset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#function to loop through all business ids, collapse all reviews for that business into one string\n",
    "if 1 == 0:\n",
    "    with codecs.open(all_user_filepath, 'w', encoding='utf_8') as f:\n",
    "        for x in user_set:\n",
    "            review_index = [i for i,j in enumerate(user) if j == x]\n",
    "            review_sub = [texts[ind] for ind in review_index]\n",
    "            review_out = ''.join(map(str, review_sub))\n",
    "            f.write(review_out + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Setup and define function for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions from modern nlp in python\n",
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_review(review_txt_filepath, review_number):\n",
    "    \"\"\"\n",
    "    retrieve a particular review index\n",
    "    from the reviews file and return it\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(it.islice(line_review(review_txt_filepath),\n",
    "                          review_number, review_number+1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model_filepath = 'lda_model_eat_30'\n",
    "trigram_dictionary_filepath = 'trigram_dict_eat_30.dict'\n",
    "trigram_model_filepath = 'trigram_model_all_eat_30'\n",
    "bigram_model_filepath = 'bigram_model_all_eat_30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore.load(lda_model_filepath)\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)\n",
    "trigram_model = Phrases.load(trigram_model_filepath)\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names = {0: u'chinese',\n",
    "               1: u'thai',\n",
    "               2: u'healthy',\n",
    "               3: u'smell',\n",
    "               4: u'japanese',\n",
    "               5: u'toronto',\n",
    "               6: u'service',\n",
    "               7: u'experience',\n",
    "               8: u'asian soup',\n",
    "               9: u'grocery',\n",
    "               10: u'parking',\n",
    "               11: u'bar ambiance',\n",
    "               12: u'uk',\n",
    "               13: u'good service',\n",
    "               14: u'fun ambiance',\n",
    "               15: u'young',\n",
    "               16: u'comfort food',\n",
    "               17: u'greek',\n",
    "               18: u'high end',\n",
    "               19: u'hotwing',\n",
    "               20: u'breakfast',\n",
    "               21: u'sweet',\n",
    "               22: u'wine & dine',\n",
    "               23: u'pubs',\n",
    "               24: u'good taste',\n",
    "               25: u'na drinks',\n",
    "               26: u'desserts',\n",
    "               27: u'coffee shop',\n",
    "               28: u'mexican',\n",
    "               29: u'reviews',\n",
    "               30: u'new york',\n",
    "               31: u'general restaurant',\n",
    "               32: u'beach',\n",
    "               33: u'location',\n",
    "               34: u'happy hour',\n",
    "               35: u'amazing',\n",
    "               36: u'vietnamese',\n",
    "               37: u'time',\n",
    "               38: u'vas legas',\n",
    "               39: u'montreal',\n",
    "               40: u'deli',\n",
    "               41: u'buffet',\n",
    "               42: u'bbq',\n",
    "               43: u'french',\n",
    "               44: u'money',\n",
    "               45: u'street taco',\n",
    "               46: u'pizza',\n",
    "               47: u'airport & delivery',\n",
    "               48: u'burger & fries',\n",
    "               49: u'italian'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#topic_names_filepath = 'topic_names_eats_30.pkl'\n",
    "\n",
    "#with open(topic_names_filepath, 'wb') as f:\n",
    "#    pickle.dump(topic_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sample_review = get_sample_review(all_test_filepath, 6)\n",
    "#print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lda_description(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_numbers = list(range(0,50))\n",
    "df_all_numbers = pd.DataFrame(columns =[\"topic_number\"])\n",
    "for topic_number in all_numbers:\n",
    "    df_all_numbers = df_all_numbers.append({\n",
    "     \"topic_number\": topic_number\n",
    "      }, ignore_index=True)\n",
    "\n",
    "\n",
    "df_topics = pd.DataFrame(columns =[\"topic_name\"])\n",
    "\n",
    "for topic_number in all_numbers:\n",
    "    df_topics = df_topics.append({\n",
    "     \"topic_name\": topic_names[topic_number]\n",
    "      }, ignore_index=True)\n",
    "#print(df_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_filepath = all_test_filepath\n",
    "\n",
    "test_df = pd.DataFrame(columns=[\"topic_name\", \"freq\", \"bus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52810"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_bus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179452"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_bus[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_bus_lda_filepath = \"review_bus_lda.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for testing individual reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlrognstad/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0,0.018699999898672104,0.0,0.0,0.0,0.0,0.045099999755620956,0.027799999341368675,0.0,0.0,0.0,0.01730000041425228,0.0,0.011900000274181366,0.011699999682605267,0.0,0.0,0.012799999676644802,0.2418999969959259,0.0,0.038100000470876694,0.0,0.0,0.0,0.10899999737739563,0.0,0.019700000062584877,0.0,0.05380000174045563,0.05090000107884407,0.0,0.03689999878406525,0.0,0.0,0.0,0.03629999980330467,0.0,0.022199999541044235,0.025699999183416367,0.0,0.0,0.0,0.009999999776482582,0.0,0.017100000753998756,0.04659999907016754,0.0,0.0,0.01600000075995922,0.020600000396370888\n"
     ]
    }
   ],
   "source": [
    "           # parse the review text with spaCy\n",
    "review_text = all_bus[752]\n",
    "\n",
    "if len(review_text) > 200000:\n",
    "    review_text = review_text[:200000]\n",
    "\n",
    "parsed_review = nlp(review_text)\n",
    " \n",
    "        # lemmatize the text and remove punctuation and whitespace\n",
    "unigram_review = [token.lemma_ for token in parsed_review\n",
    "    if not punct_space(token)]\n",
    "    \n",
    "        # apply the first-order and secord-order phrase models\n",
    "bigram_review = bigram_model[unigram_review]\n",
    "trigram_review = trigram_model[bigram_review]\n",
    "    \n",
    "        # remove any remaining stopwords\n",
    "trigram_review = [term for term in trigram_review\n",
    "    if not term in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "    \n",
    "        # create a bag-of-words representation\n",
    "review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "    \n",
    "         # create an LDA representation\n",
    "review_lda = lda[review_bow]\n",
    "    \n",
    "        # sort with the most highly related topics first\n",
    "review_lda = sorted(review_lda, key=lambda review_lda: -review_lda[1])\n",
    "        \n",
    "df = pd.DataFrame(columns=[\"topic_number\", \"freq\"])\n",
    "\n",
    "for topic_number, freq in review_lda:\n",
    "    df = df.append({\n",
    "    \"topic_number\": topic_number,\n",
    "    \"freq\":  round(freq, 4)\n",
    "    }, ignore_index=True)\n",
    "            #merge with complete topic list and replace na with zero\n",
    "df_full = pd.merge(df_all_numbers, df, how='left', on=['topic_number'])\n",
    "df_full = df_full.fillna(0)\n",
    "       \n",
    "one_row = df_full['freq']\n",
    "        \n",
    "out = one_row.values.tolist()\n",
    "\n",
    "out2 = str(out)\n",
    "out3 = re.sub(r\"[\\[ | \\]]\", \"\", out2)\n",
    "\n",
    "print(out3)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output_file = open('review_bus_lda.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlrognstad/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22 s, sys: 14.7 s, total: 36.7 s\n",
      "Wall time: 7.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#function to loop through all businesses and convert review to \n",
    "with open('output_file_test.csv', 'w') as file:\n",
    "    wr = csv.writer(file, dialect = 'excel')\n",
    "    for busi in list(range(0,10)):\n",
    "        # parse the review text with spaCy\n",
    "        parsed_review = nlp(all_bus[busi])\n",
    " \n",
    "        # lemmatize the text and remove punctuation and whitespace\n",
    "        unigram_review = [token.lemma_ for token in parsed_review\n",
    "                      if not punct_space(token)]\n",
    "    \n",
    "        # apply the first-order and secord-order phrase models\n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "    \n",
    "        # remove any remaining stopwords\n",
    "        trigram_review = [term for term in trigram_review\n",
    "                      if not term in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "    \n",
    "        # create a bag-of-words representation\n",
    "        review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "    \n",
    "         # create an LDA representation\n",
    "        review_lda = lda[review_bow]\n",
    "    \n",
    "        # sort with the most highly related topics first\n",
    "        review_lda = sorted(review_lda, key=lambda review_lda: -review_lda[1])\n",
    "        \n",
    "        bus = ''.join([list(x) for x in bus_set][busi])\n",
    "\n",
    "        df = pd.DataFrame(columns=[\"topic_number\", \"freq\"])\n",
    "\n",
    "        for topic_number, freq in review_lda:\n",
    "            df = df.append({\n",
    "            \"topic_number\": topic_number,\n",
    "            \"freq\":  round(freq, 4)\n",
    "            }, ignore_index=True)\n",
    "            #merge with complete topic list and replace na with zero\n",
    "        df_full = pd.merge(df_all_numbers, df, how='left', on=['topic_number'])\n",
    "        df_full = df_full.fillna(0)\n",
    "        \n",
    "        df_full['bus'] = bus\n",
    "\n",
    "        pivoted = df_full.pivot('bus', 'topic_number')\n",
    "        \n",
    "        one_row = pivoted.iloc[0]\n",
    "        \n",
    "        out = one_row.values.tolist()\n",
    "        out.insert(0, bus)\n",
    "        out2 = str(out)\n",
    "        out3 = re.sub(r\"[\\[ | \\]]\", \"\", out2)\n",
    "            \n",
    "            #print(busi)\n",
    "        wr.writerow(out)    \n",
    "        #output_file.write(out3)\n",
    "        #output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "RESULT = out#['apple','cherry','orange','pineapple','strawberry']\n",
    "with open(\"output.csv\",'w') as resultFile:\n",
    "    wr = csv.writer(resultFile, dialect='excel')\n",
    "    wr.writerow(RESULT)\n",
    "    wr.writerow(RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(out, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"21\" halign=\"left\">freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_number</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bus</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9yxHYyXMHjh7LpoolJiH9w</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0809</td>\n",
       "      <td>0.0243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       freq                                                  \\\n",
       "topic_number             0       1    2    3       4    5    6      7    8    \n",
       "bus                                                                           \n",
       "9yxHYyXMHjh7LpoolJiH9w  0.0  0.0673  0.0  0.0  0.0574  0.0  0.0  0.096  0.0   \n",
       "\n",
       "                               ...                                          \\\n",
       "topic_number                9  ...    40      41      42   43      44   45   \n",
       "bus                            ...                                           \n",
       "9yxHYyXMHjh7LpoolJiH9w  0.0572 ...   0.0  0.0809  0.0243  0.0  0.0577  0.0   \n",
       "\n",
       "                                            \n",
       "topic_number             46   47   48   49  \n",
       "bus                                         \n",
       "9yxHYyXMHjh7LpoolJiH9w  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1 rows x 50 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_read = pd.io.parsers.read_csv('output_file_test.csv',sep=\",\")\n",
    "len(test_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4727</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4728</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4729</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4730</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4731</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4732</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4733</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4734</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4735</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4737</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4738</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4739</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4740</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756</th>\n",
       "      <td>]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4757 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      [\n",
       "0     '\n",
       "1     M\n",
       "2     a\n",
       "3     I\n",
       "4     _\n",
       "5     E\n",
       "6     e\n",
       "7     v\n",
       "8     M\n",
       "9     b\n",
       "10    W\n",
       "11    B\n",
       "12    U\n",
       "13    n\n",
       "14    K\n",
       "15    3\n",
       "16    6\n",
       "17    L\n",
       "18    4\n",
       "19    p\n",
       "20    A\n",
       "21    n\n",
       "22    g\n",
       "23    '\n",
       "24    ,\n",
       "25    0\n",
       "26    .\n",
       "27    0\n",
       "28    ,\n",
       "29    0\n",
       "...  ..\n",
       "4727  0\n",
       "4728  7\n",
       "4729  0\n",
       "4730  3\n",
       "4731  3\n",
       "4732  3\n",
       "4733  4\n",
       "4734  8\n",
       "4735  1\n",
       "4736  ,\n",
       "4737  0\n",
       "4738  .\n",
       "4739  0\n",
       "4740  ,\n",
       "4741  0\n",
       "4742  .\n",
       "4743  0\n",
       "4744  ,\n",
       "4745  0\n",
       "4746  .\n",
       "4747  0\n",
       "4748  ,\n",
       "4749  0\n",
       "4750  .\n",
       "4751  0\n",
       "4752  ,\n",
       "4753  0\n",
       "4754  .\n",
       "4755  0\n",
       "4756  ]\n",
       "\n",
       "[4757 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlrognstad/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#function to loop through all businesses and convert review to \n",
    "if 1 == 1:\n",
    "    for busi in list(range(0,10)):\n",
    "        # parse the review text with spaCy\n",
    "        parsed_review = nlp(all_bus[busi])\n",
    " \n",
    "        # lemmatize the text and remove punctuation and whitespace\n",
    "        unigram_review = [token.lemma_ for token in parsed_review\n",
    "                      if not punct_space(token)]\n",
    "    \n",
    "        # apply the first-order and secord-order phrase models\n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "    \n",
    "        # remove any remaining stopwords\n",
    "        trigram_review = [term for term in trigram_review\n",
    "                      if not term in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "    \n",
    "        # create a bag-of-words representation\n",
    "        review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "    \n",
    "         # create an LDA representation\n",
    "        review_lda = lda[review_bow]\n",
    "    \n",
    "        # sort with the most highly related topics first\n",
    "        review_lda = sorted(review_lda, key=lambda review_lda: -review_lda[1])\n",
    "        \n",
    "        bus = ''.join([list(x) for x in bus_set][busi])\n",
    "\n",
    "        df = pd.DataFrame(columns=[\"topic_number\", \"freq\"])\n",
    "\n",
    "        for topic_number, freq in review_lda:\n",
    "            df = df.append({\n",
    "            \"topic_number\": topic_number,\n",
    "            \"freq\":  round(freq, 4)\n",
    "            }, ignore_index=True)\n",
    "            #merge with complete topic list and replace na with zero\n",
    "        df_full = pd.merge(df_all_numbers, df, how='left', on=['topic_number'])\n",
    "        df_full = df_full.fillna(0)\n",
    "        \n",
    "        df_full['bus'] = bus\n",
    "\n",
    "        pivoted = df_full.pivot('bus', 'topic_number')\n",
    "        \n",
    "        one_row = pivoted.iloc[0]\n",
    "        \n",
    "        out = one_row.values.tolist()\n",
    "        out.insert(0, bus)\n",
    "        out2 = str(out)\n",
    "        out3 = re.sub(r\"[\\[ | \\]]\", \"\", out2)\n",
    "            \n",
    "            #print(busi)\n",
    "           \n",
    "        output_file.write(out3)\n",
    "        output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365557"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_read = pd.io.parsers.read_csv('output_file.csv',sep=\",\")\n",
    "len(test_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_read = pd.io.parsers.read_csv(all_bus_lda_filepath,sep=\",\")\n",
    "len(test_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365557, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_read.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlrognstad/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min 33s, sys: 14min 32s, total: 40min 6s\n",
      "Wall time: 7min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#function to loop through all businesses and convert review to \n",
    "if 1 == 1:\n",
    "    with open(all_bus_lda_filepath, 'w') as f:\n",
    "        for busi in list(range(0,500)):\n",
    "            # parse the review text with spaCy\n",
    "            parsed_review = nlp(all_bus[busi])\n",
    " \n",
    "            # lemmatize the text and remove punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                      if not punct_space(token)]\n",
    "    \n",
    "            # apply the first-order and secord-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "    \n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review\n",
    "                      if not term in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "    \n",
    "            # create a bag-of-words representation\n",
    "            review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "    \n",
    "            # create an LDA representation\n",
    "            review_lda = lda[review_bow]\n",
    "    \n",
    "            # sort with the most highly related topics first\n",
    "            review_lda = sorted(review_lda, key=lambda review_lda: -review_lda[1])\n",
    "        \n",
    "            bus = ''.join([list(x) for x in bus_set][busi])\n",
    "\n",
    "            df = pd.DataFrame(columns=[\"topic_number\", \"freq\"])\n",
    "\n",
    "            for topic_number, freq in review_lda:\n",
    "                df = df.append({\n",
    "                \"topic_number\": topic_number,\n",
    "                \"freq\":  round(freq, 4)\n",
    "                }, ignore_index=True)\n",
    "            #merge with complete topic list and replace na with zero\n",
    "            df_full = pd.merge(df_all_numbers, df, how='left', on=['topic_number'])\n",
    "            df_full = df_full.fillna(0)\n",
    "        \n",
    "            df_full['bus'] = bus\n",
    "\n",
    "            pivoted = df_full.pivot('bus', 'topic_number')\n",
    "        \n",
    "            one_row = pivoted.iloc[0]\n",
    "        \n",
    "            out = one_row.values.tolist()\n",
    "            out.insert(0, bus)\n",
    "            out2 = str(out)\n",
    "            out3 = re.sub(r\"[\\[ | \\]]\", \"\", out2)\n",
    "            \n",
    "            #print(busi)\n",
    "           \n",
    "            f.write(out3)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_read = pd.io.parsers.read_csv(all_bus_lda_filepath,sep=\",\")\n",
    "len(test_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlrognstad/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "for rd in list(range(1,99)):\n",
    "    st = rd * 500\n",
    "    ed = (rd + 1) * 500\n",
    "    with open(all_bus_lda_filepath, 'a') as f:\n",
    "        for busi in list(range(st,ed)):\n",
    "            # parse the review text with spaCy\n",
    "            parsed_review = nlp(all_bus[busi])\n",
    "    \n",
    "            # lemmatize the text and remove punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                      if not punct_space(token)]\n",
    "    \n",
    "            # apply the first-order and secord-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "    \n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review\n",
    "                      if not term in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "    \n",
    "            # create a bag-of-words representation\n",
    "            review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "    \n",
    "            # create an LDA representation\n",
    "            review_lda = lda[review_bow]\n",
    "    \n",
    "            # sort with the most highly related topics first\n",
    "            review_lda = sorted(review_lda, key=lambda review_lda: -review_lda[1])\n",
    "        \n",
    "            bus = ''.join([list(x) for x in bus_set][busi])\n",
    "\n",
    "            df = pd.DataFrame(columns=[\"topic_number\", \"freq\"])\n",
    "\n",
    "            for topic_number, freq in review_lda:\n",
    "                df = df.append({\n",
    "                \"topic_number\": topic_number,\n",
    "                \"freq\":  round(freq, 4)\n",
    "                }, ignore_index=True)\n",
    "            #merge with complete topic list and replace na with zero\n",
    "            df_full = pd.merge(df_all_numbers, df, how='left', on=['topic_number'])\n",
    "            df_full = df_full.fillna(0)\n",
    "        \n",
    "            df_full['bus'] = bus\n",
    "\n",
    "            pivoted = df_full.pivot('bus', 'topic_number')\n",
    "        \n",
    "            one_row = pivoted.iloc[0]\n",
    "        \n",
    "            out = one_row.values.tolist()\n",
    "            out.insert(0, bus)\n",
    "            out2 = str(out)\n",
    "            out3 = re.sub(r\"[\\[ | \\]]\", \"\", out2)\n",
    "            \n",
    "           \n",
    "            f.write(out3)\n",
    "            f.write('\\n')\n",
    "            \n",
    "    print(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1500 2000\n"
     ]
    }
   ],
   "source": [
    "rd = 3\n",
    "st = rd * 500\n",
    "ed = (rd + 1) * 500\n",
    "print(rd, st, ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "out3 = re.sub(r\"[\\[ | \\]]\", \"\", out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'ToNd6fEn_SvcQc1Fulsidg',0.09790000319480896,0.0,0.0,0.0,0.21660000085830688,0.0,0.10610000044107437,0.016300000250339508,0.0,0.011699999682605267,0.014100000262260437,0.0,0.0,0.0,0.014700000174343586,0.0,0.01209999993443489,0.032999999821186066,0.0,0.0,0.0,0.0,0.0,0.020999999716877937,0.010400000028312206,0.0,0.021900000050663948,0.0,0.0,0.052000001072883606,0.0,0.03269999846816063,0.0,0.013899999670684338,0.0,0.0,0.0,0.04740000143647194,0.0,0.0,0.0,0.15690000355243683,0.0,0.0,0.016100000590085983,0.0,0.0,0.019200000911951065,0.0,0.0\""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['1ylA7yyrMMUX1zcu5EqO4Q', 0.0, 0.0, 0.0430000014603138, 0.012299999594688416, 0.0, 0.018699999898672104, 0.0, 0.01209999993443489, 0.0, 0.017799999564886093, 0.0, 0.01889999955892563, 0.0, 0.010200000368058681, 0.0560000017285347, 0.0, 0.30070000886917114, 0.0, 0.0, 0.0, 0.02810000069439411, 0.0, 0.0, 0.03150000050663948, 0.0348999984562397, 0.0, 0.0, 0.0, 0.0, 0.07020000368356705, 0.0, 0.027400000020861626, 0.0, 0.02290000021457672, 0.0, 0.035100001841783524, 0.0, 0.05079999938607216, 0.025299999862909317, 0.0203000009059906, 0.0, 0.0, 0.0284000001847744, 0.0, 0.034699998795986176, 0.0, 0.0, 0.016499999910593033, 0.0, 0.0]\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = str(out)\n",
    "''.join(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlrognstad/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "busi =0\n",
    "parsed_review = nlp(all_bus[busi])\n",
    "unigram_review = [token.lemma_ for token in parsed_review\n",
    "                      if not punct_space(token)]\n",
    "bigram_review = bigram_model[unigram_review]\n",
    "trigram_review = trigram_model[bigram_review]\n",
    "trigram_review = [term for term in trigram_review\n",
    "    if not term in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "review_lda = lda[review_bow]\n",
    "\n",
    "bus = ''.join([list(x) for x in bus_set][busi])\n",
    "\n",
    "df = pd.DataFrame(columns=[\"topic_number\", \"freq\"])\n",
    "\n",
    "for topic_number, freq in review_lda:\n",
    "    df = df.append({\n",
    "    \"topic_number\": topic_number,\n",
    "    \"freq\":  round(freq, 4)\n",
    "    }, ignore_index=True)\n",
    "#merge with complete topic list and replace na with zero\n",
    "df_full = pd.merge(df_all_numbers, df, how='left', on=['topic_number'])\n",
    "df_full = df_full.fillna(0)\n",
    "        \n",
    "df_full['bus'] = bus\n",
    "\n",
    "pivoted = df_full.pivot('bus', 'topic_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"21\" halign=\"left\">freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_number</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bus</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1ylA7yyrMMUX1zcu5EqO4Q</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>0.0886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       freq                                                  \\\n",
       "topic_number             0    1    2       3    4    5    6    7    8    9    \n",
       "bus                                                                           \n",
       "1ylA7yyrMMUX1zcu5EqO4Q  0.0  0.0  0.0  0.0624  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                         ...                                                \\\n",
       "topic_number             ...     40   41   42      43      44   45      46   \n",
       "bus                      ...                                                 \n",
       "1ylA7yyrMMUX1zcu5EqO4Q   ...    0.0  0.0  0.0  0.0171  0.0269  0.0  0.0592   \n",
       "\n",
       "                                             \n",
       "topic_number                47   48      49  \n",
       "bus                                          \n",
       "1ylA7yyrMMUX1zcu5EqO4Q  0.0886  0.0  0.1216  \n",
       "\n",
       "[1 rows x 50 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_row = pivoted.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1ylA7yyrMMUX1zcu5EqO4Q', 0.0, 0.0, 0.0, 0.06239999830722809, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02630000002682209, 0.0674000009894371, 0.0, 0.023099999874830246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11969999969005585, 0.0, 0.21979999542236328, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06069999933242798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022199999541044235, 0.07370000332593918, 0.0, 0.0, 0.0, 0.017100000753998756, 0.026900000870227814, 0.0, 0.05920000001788139, 0.08860000222921371, 0.0, 0.12160000205039978]\n"
     ]
    }
   ],
   "source": [
    "out = one_row.values.tolist()\n",
    "out.insert(0, bus)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This place is so good. I work nearby and come here often for lunch.  Their menu is 7.49 for one veg & one meat & rice or noodles, 7.99 for two veg & one meat & rice or noodles, 8.49 for one veg & two meat & rice or noodles. They also offer salads (7.49 I believe).  The rice is amazing. They have a few different kinds, but I have only tried the Jambalayan kind. It's sort of spicy but SO delicious. It is, in fact, so good, I am not willing to try the other kinds. Instead of rice, you can also get noodles.  Their vegetables include home fries, corn, mixed veggies, zucchini, mashed potatoes, and a few others. I have tried the home fries, mixed veggies, and zucchini and they are all good, but I personally like the home fries the most.  Their meats change on a daily basis, but they normally have bourbon chicken, blackened chicken, honey glazed chicken, spicy beef, rainbow shrimp, a type of fish (I forget), and a few other chicken flavours. The bourbon chicken is amazing. Ask for extra sauce over your rice.   The women here work fast, and will give you a sample if you are unsure of what you want. The servings are quite large. I find it's not the regular 'greasy Chinese food' you normally get, and instead, is probably decently healthy (you're in a food court, it's relative).   The drinks are a bit expensive, so that's why they get 4 stars. If you can, buy your food here and then go to KFC or somewhere to get a cheaper drink. I also wish the meals were about 50 cents cheaper. It's still good food for the price, so I won't complain too much. Definitely give it a try.Came here around 8:30pm which is when they start offering their end of day special.  I the Blackened fish, battered shrimp, mango salad and fried rice combo box $6.29.  The fish was very good.  Nice pepper taste and moist.  Mango salad was good too.  Shrimp was over cooked and fried rice was bland.  If you want cheap Cajun and Chinese meal then this is the place for you.\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DGwDXazeFcD7DByweszpFA'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([list(x) for x in bus_set][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
