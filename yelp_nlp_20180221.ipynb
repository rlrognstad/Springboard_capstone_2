{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regualr spacy instal lwould not work, had to get from github\n",
    "#conda install -c conda-forge spacy\n",
    "#python -m spacy download en\n",
    "#check with: python -m spacy validate\n",
    "#restart jupyter notebook\n",
    "\n",
    "import codecs\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "#import en_core_web_md\n",
    "#nlp = en_core_web_md.load()\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I live very near this place and have been curious to try it for sometime now as it always seems busy.\n",
      "\n",
      "Well my curiosity has been satisfied and I'm sure I'll never feel the need to visit this place again.\n",
      "\n",
      "It struck me as a typical trendy place that yuppies feel they need to visit as all the other yuppies do.  \n",
      "\"Oh the food is sooo good!!\"\n",
      "\"Really? You've eaten there?\"\n",
      "\"No. But all my yuppy friends have and they say it's good so it must be.\"\n",
      "\n",
      "I only give it a two because the service and presentation of the food was decent.\n",
      "\n",
      "We ordered something called oxtail fries which were horrible. I'm sure an actual oxtail would probably have been tastier.\n",
      "\n",
      "My girlfriend and I each ordered a different burger so we could sample each others. Sorry I can't remember what they were called but we didn't care for either one. The best compliment I could pay them is to say they were at least edible.\n",
      "\n",
      "So as mentioned the service was decent and the food looked presentable but as far as taste I can't recommend this place. \n",
      "\n",
      "Perhaps it's just me. Maybe the asian fusion just isn't my type of food. \n",
      "\n",
      "I'd rather have a Wendys burger over this place.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "with codecs.open('review_text_lv_rest_subset_30.txt', encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "        \n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions from modern nlp in python\n",
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = 'unigram_sentences_all_eat_30.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2d 14h 54min 22s, sys: 1d 16h 18min 7s, total: 4d 7h 12min 29s\n",
      "Wall time: 15h 9min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if 1 == 0:\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus('review_text_eats_subset_30.txt'):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17h 53min 55s, sys: 12h 6min 47s, total: 1d 6h 43s\n",
      "Wall time: 4h 29min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if 1 == 0:\n",
    "    with codecs.open('unigram_sentences_all_lv_rest_30.txt', 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus('review_text_lv_rest_subset_30.txt'):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)\n",
    "\n",
    "#build models with larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at first -PRON- be not impress that -PRON- be\n",
      "\n",
      "a 45 min wait however -PRON- bf really want to try this piece so -PRON- decide to wait\n",
      "\n",
      "-PRON- be definitely different and worth the wait\n",
      "\n",
      "-PRON- start off with the house cured meat -PRON- be good but definitely too much for just 2 people to share and no need to order the extra bread if -PRON- be only two people\n",
      "\n",
      "next come the roasted bone marrow which be amazing and definitely worth ordering\n",
      "\n",
      "have -PRON- not order so many other dish -PRON- would have order a second round of this\n",
      "\n",
      "who know bone marrow could be this tasty\n",
      "\n",
      "then come the spicy horse tartare which -PRON- think be intriguing as -PRON- have never hear so -PRON- have order -PRON-\n",
      "\n",
      "when -PRON- show up -PRON- be like a salad\n",
      "\n",
      "be not bad worth try but probably only a one time thing for -PRON-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print( u' '.join(unigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model_filepath = 'bigram_model_all_eat_30'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 49s, sys: 5.43 s, total: 8min 54s\n",
      "Wall time: 8min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#time consuming\n",
    "if 1 == 1:\n",
    "\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = 'bigram_sentences_all_eat_30.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlrognstad/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 41s, sys: 5.7 s, total: 19min 47s\n",
      "Wall time: 19min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            \n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at first -PRON- be not impress that -PRON- be\n",
      "\n",
      "a 45_min wait however -PRON- bf really want to try this piece so -PRON- decide to wait\n",
      "\n",
      "-PRON- be definitely different and worth the wait\n",
      "\n",
      "-PRON- start off with the house_cured meat -PRON- be good but definitely too much for just 2 people to share and no need to order the extra bread if -PRON- be only two people\n",
      "\n",
      "next come the roasted bone_marrow which be amazing and definitely worth ordering\n",
      "\n",
      "have -PRON- not order so many other dish -PRON- would have order a second round of this\n",
      "\n",
      "who know bone_marrow could be this tasty\n",
      "\n",
      "then come the spicy horse_tartare which -PRON- think be intriguing as -PRON- have never hear so -PRON- have order -PRON-\n",
      "\n",
      "when -PRON- show up -PRON- be like a salad\n",
      "\n",
      "be not bad worth try but probably only a one time thing for -PRON-\n",
      "\n",
      "tongue on brioche be definitely another worth order\n",
      "\n",
      "-PRON- be a sandwich that taste like a montreal smoke meat sandwich\n",
      "\n",
      "so good that -PRON- melt in -PRON- mouth so soft and tender\n",
      "\n",
      "if -PRON- be not because -PRON- have order so much food and this be the last main_course that show up -PRON- would have devour -PRON-\n",
      "\n",
      "for dessert -PRON- have order the foie and nutella as -PRON- think this be very interesting\n",
      "\n",
      "-PRON- be good but after 4 dish already this be just a little too rich\n",
      "\n",
      "-PRON- come with banana bread that be really good\n",
      "\n",
      "overall definitely worth the try\n",
      "\n",
      "just make sure -PRON- bring cash =)\n",
      "\n",
      "omg come here with some friend for brunch\n",
      "\n",
      "and wow\n",
      "\n",
      "-PRON- do have to wait for about 30_min for a table for 4 but -PRON- be later in the day\n",
      "\n",
      "look at -PRON- menu -PRON- want to try everything\n",
      "\n",
      "the french_toast the strawberry_shortcake donut -PRON- be sell out by the time -PRON- get there\n",
      "\n",
      "but -PRON- wind up only order two thing the classic 2 egg any style house cure bacon or sage and onion sausage crispy griddle red skin potato with sea_salt and thyme\n",
      "\n",
      "serve with toast of -PRON- choice\n",
      "\n",
      "upgrade -PRON- toast to a buttermilk_biscuit\n",
      "\n",
      "emma 's benny warm and fluffy buttermilk_biscuit top with house cure peameal_bacon poach_egg and smother in mousselline sauce\n",
      "\n",
      "serve with a side of griddled red skin potato\n",
      "\n",
      "big portion really good if -PRON- want the benny make sure -PRON- ask -PRON- if -PRON- still have -PRON- as -PRON- tend to sell out quick -PRON- have order the last one of the day\n",
      "\n",
      "great japanese restaurant in the bloor and church area\n",
      "\n",
      "be come here since the first open year_ago\n",
      "\n",
      "food have always be good and fresh\n",
      "\n",
      "however if -PRON- be here for lunch -PRON- will take an hour and make sure -PRON- come a few minute earlier otherwise -PRON- may be a long wait as -PRON- be small inside\n",
      "\n",
      "-PRON- have some pretty interesting thing here\n",
      "\n",
      "definitely a one stop shop for cheese and bread =_p also carry a variety of pate that be pretty tasty\n",
      "\n",
      "love this place\n",
      "\n",
      "the cake be delicious but really rich\n",
      "\n",
      "-PRON- have cute cupcake and lot of macaroon\n",
      "\n",
      "the good part -PRON- now have a gluten_free chocolate cake which hubby and -PRON- think be good than the regular cake\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 230, 270):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model_filepath = 'trigram_model_all_eat_30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 55s, sys: 2.22 s, total: 7min 58s\n",
      "Wall time: 7min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "    trigram_model.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = 'trigram_sentences_all_eat_30.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlrognstad/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 36s, sys: 5.76 s, total: 19min 41s\n",
      "Wall time: 19min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            \n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stumble_upon this one day thought -PRON- would give -PRON- a try\n",
      "\n",
      "typical sushi place sushi be pretty good\n",
      "\n",
      "price be decent\n",
      "\n",
      "party tray be decent\n",
      "\n",
      "=)\n",
      "\n",
      "fish be good really good\n",
      "\n",
      "love how -PRON- could have so many different option for fish choice\n",
      "\n",
      "chip be only ok\n",
      "\n",
      "onion_ring be not the same as the regular onion a little too crispy\n",
      "\n",
      "-PRON- have order the house organic batter mahi_mahi very light not really heavy\n",
      "\n",
      "creamy coleslaw be flavourless\n",
      "\n",
      "share the o.t.h poutine which be ok but not for -PRON-\n",
      "\n",
      "will go back to try the pull_pork or duck_confit one\n",
      "\n",
      "good for gluten_free\n",
      "\n",
      "=)\n",
      "\n",
      "after read the review on yelp -PRON- boyfriend and i decide to give this place a try especially since -PRON- be on a mission to try raman in every city -PRON- visit\n",
      "\n",
      "-PRON- be really excited as when -PRON- be in nyc -PRON- have really good raman\n",
      "\n",
      "listen to the review -PRON- start off with the gyoza appetizer which be not as good as everyone say -PRON- be\n",
      "\n",
      "-PRON- be pan_fry with the thin_layer of skin but there be an aftertaste of soap water\n",
      "\n",
      "which -PRON- should have know since the glass of water -PRON- serve -PRON- be disgusting\n",
      "\n",
      "-PRON- have order the jumbo sumo raman beware -PRON- be a very big bowl of noodle which -PRON- do not expect\n",
      "\n",
      "the sumo raman come with a half boil_egg seaweed pork_belly and other veggie\n",
      "\n",
      "the noodle and pork_belly be good\n",
      "\n",
      "soup be tasteless\n",
      "\n",
      "definitely would not go back again\n",
      "\n",
      "come here for a friend 's birthday\n",
      "\n",
      "-PRON- have look -PRON- up and think -PRON- sound good\n",
      "\n",
      "first of all when -PRON- make reservation -PRON- only have a 7_p.m. time slot for -PRON- -PRON- be look for 8_p.m. and when -PRON- be there -PRON- be seat in the low area of the restaurant which be pretty empty\n",
      "\n",
      "there be an extensive menu everything on the menu sound good but when -PRON- get -PRON- food there be nothing_special especially for what -PRON- pay for -PRON-\n",
      "\n",
      "the pizza 's be huge portion so if -PRON- order -PRON- make sure -PRON- share -PRON-\n",
      "\n",
      "there be 3 risotto on the menu the seasonal_vegetable one be the good one\n",
      "\n",
      "some of the pasta be portion differently some be huge other be tiny\n",
      "\n",
      "so be careful\n",
      "\n",
      "the food come out really fast after -PRON- order so fast that some of the food be cold not sure if -PRON- make -PRON- in advance and just heat up afterwards\n",
      "\n",
      "the dessert take_forever to come\n",
      "\n",
      "at the end of the night when the bill come there be a water service charge $_12.50 for 9 people for just get tap water\n",
      "\n",
      "and 18_gratuity after tax\n",
      "\n",
      "atmosphere be good but other than that -PRON- do not recommend this place at all\n",
      "\n",
      "come here after dinner to meet up with some friend\n",
      "\n",
      "-PRON- be nice and roomy for 7 people\n",
      "\n",
      "-PRON- order a plate of rosemary fry to share which be not bad however when serve -PRON- be room_temperature and do not taste like -PRON- just come out of the fryer\n",
      "\n",
      "bf have order a couple of beer which -PRON- be not impressed at all with\n",
      "\n",
      "want to give this place a try since -PRON- be in -PRON- neighbourhood and -PRON- would not have to go far\n",
      "\n",
      "come here for a late lunch on a friday there be a few table but nothing crazy\n",
      "\n",
      "-PRON- have order spicy_miso_ramen salt broth\n",
      "\n",
      "ramen\n",
      "\n",
      "-PRON- be ok can not say there be great and can not say -PRON- be terrible\n",
      "\n",
      "the noodle have a nice texture to -PRON- a bit chewy and not too soft\n",
      "\n",
      "but the broth be a bit bland definitely lack some depth to -PRON- like another reviewer have say\n",
      "\n",
      "look at the menu -PRON- definitely do have quite a few raman option perhaps too many option and -PRON- should try to narrow -PRON- down to make -PRON- good\n",
      "\n",
      "perhaps next time -PRON- should try the other menu item and not the raman\n",
      "\n",
      "decide to give this place a try base on the review\n",
      "\n",
      "service be really slow on a thursday_afternoon work day\n",
      "\n",
      "-PRON- take almost 30_min before -PRON- get -PRON- food\n",
      "\n",
      "definitely not the place to go during the work week\n",
      "\n",
      "order a pork_bone soup which be just ok\n",
      "\n",
      "nothing_special or memorable about -PRON-\n",
      "\n",
      "not sure if -PRON- would come back again\n",
      "\n",
      "this place be really good\n",
      "\n",
      "the good restaurant in the area\n",
      "\n",
      "the food there be really tasty especially the ahi_tuna club sandwich lobster_ravioli and the tandoori_chicken flat_bread\n",
      "\n",
      "-PRON- also have mock chicken for those who be vegetarian\n",
      "\n",
      "gorgeous patio for the summer time\n",
      "\n",
      "decide to give this place a try since -PRON- be in the neighbourhood\n",
      "\n",
      "love that there be no line up at all\n",
      "\n",
      "however come here at 5_p.m. on saturday_afternoon and only original flavour be leave\n",
      "\n",
      "-PRON- have three flavour to choose from original chocolate and coffee\n",
      "\n",
      "-PRON- nephew love -PRON- and think -PRON- be good than uncle_tetsu which -PRON- have not yet try\n",
      "\n",
      "small little japanese restaurant in the don_mills neighbourhood\n",
      "\n",
      "lot of different roll to pick from\n",
      "\n",
      "great lunch special selection\n",
      "\n",
      "service be a little slow though\n",
      "\n",
      "so if -PRON- only have 1 hour for lunch may not be the good place\n",
      "\n",
      "take out be pretty quick though\n",
      "\n",
      "chinese version of cheesecake_factory\n",
      "\n",
      "come here with a friend on a thursday_night at 7:30pm and be tell -PRON- be a 45_min wait\n",
      "\n",
      "-PRON- friend have walk by at 6_p.m. to try to make a reservation in person but -PRON- say -PRON- would not take -PRON-\n",
      "\n",
      "-PRON- wind up sit at the bar to bypass the wait as -PRON- serve the full menu at the bar as well\n",
      "\n",
      "nothing crazy different on the menu but the bartender be really nice as -PRON- have sit there for over 2 hour\n",
      "\n",
      "-PRON- have order the crispy green_bean which be really tasty\n",
      "\n",
      "ma_pao tofu which be weird that -PRON- be crispy tofu serve on top of a bed of broccoli\n",
      "\n",
      "vegetarian fry rice ok but also serve with a lot of broccoli\n",
      "\n",
      "will be back to try some of -PRON- noodle dish\n",
      "\n",
      "-PRON- be here on a holiday monday where nothing much be open\n",
      "\n",
      "-PRON- have to grab some takeout as -PRON- be in a rush and this be the only place open\n",
      "\n",
      "-PRON- order beef noodle soup grill pork_chop rice grill port vermicelli the food be ok nothing exciting and kind of bland\n",
      "\n",
      "read about this bakery on blogto so decide to check -PRON- out since -PRON- be so close\n",
      "\n",
      "cute little place in the mid of chinatown\n",
      "\n",
      "smelt amazing when -PRON- walk in\n",
      "\n",
      "walk out with the following carrot_cake ginger spice cookie spicy chocolate cookie\n",
      "\n",
      "gluten_free peanut_butter cookie\n",
      "\n",
      "the mini 5 inch cake be a good size not overly price at $ 7\n",
      "\n",
      "be a bit on the sweet side but be not terrible\n",
      "\n",
      "just make sure -PRON- have -PRON- with tea or coffee with no sugar\n",
      "\n",
      "the cookie be $_1.75 a piece average for the size of the cookie\n",
      "\n",
      "however -PRON- be the crunchy kind of cookie so if -PRON- be look for a soft chewy cookie -PRON- should skip -PRON-\n",
      "\n",
      "overall not bad will be back to try other stuff\n",
      "\n",
      "can not wait until -PRON- add more gluten_free item on the menu\n",
      "\n",
      "nice addition to the distillery_district\n",
      "\n",
      "stumble_upon this place when -PRON- be out check the light festival\n",
      "\n",
      "nice place for -PRON- to hang out and get a coffee tea or hot chocolate\n",
      "\n",
      "-PRON- have a few flavour for hot chocolate good part -PRON- can get -PRON- in a dairy_free option\n",
      "\n",
      "-PRON- have try the peppermint hot chocolate with soy_milk and -PRON- be nice and minty\n",
      "\n",
      "in look at the menu -PRON- could also grab a bite to eat or walk in -PRON- store to look at the different condiment -PRON- sell\n",
      "\n",
      "after read review on this place think -PRON- would take -PRON- family here to give -PRON- a try\n",
      "\n",
      "but after drive in circle a few time -PRON- realize that this place have close down could not find -PRON- at all\n",
      "\n",
      "cute place in danforth and coxwell area\n",
      "\n",
      "-PRON- have a nice patio to sit which be also dog friendly\n",
      "\n",
      "-PRON- have try the wild_boar pizza which be very good\n",
      "\n",
      "definitely want to try the other item on the menu\n",
      "\n",
      "friendly service\n",
      "\n",
      "definitely will be back here more often since -PRON- be so close to -PRON-\n",
      "\n",
      "-PRON- can see that people only eat here to use the voucher -PRON- have purchase as -PRON- seem to always have some sort of deal available unless -PRON- come early enough make sure -PRON- make reservation otherwise -PRON- will take up to 30_min to get seat for a table of 2\n",
      "\n",
      "the food be ok\n",
      "\n",
      "not the great -PRON- have have but definitely not the bad\n",
      "\n",
      "for the price -PRON- be pay $_15 for $_30 worth of food\n",
      "\n",
      "-PRON- be ok\n",
      "\n",
      "definitely only visit if -PRON- have purchase a voucher\n",
      "\n",
      "-PRON- sister be a big fan of the noodle here as -PRON- do have some soup option with no msg\n",
      "\n",
      "the way the system work be that -PRON- be a combo soup base option no msg fish soup or chicken soup base two topping be include with additional topping at $_1.75 each type of noodle 1 hot drink be include but other option have an additional_charge then -PRON- also have an option of add side item\n",
      "\n",
      "-PRON- have order the parsley and century_egg fish soup with fish filet fish tofu cabbage and flat rice noodle\n",
      "\n",
      "not sure if -PRON- be due to the type of noodle i have pick\n",
      "\n",
      "but i find -PRON- a little lacky in flavour\n",
      "\n",
      "otherwise -PRON- be not bad pretty big portion\n",
      "\n",
      "-PRON- would also upgrade to a cold drink next time since the hot lemon tea that i have get be not really that great\n",
      "\n",
      "this be a most stop for -PRON- and -PRON- girlfriend whenever -PRON- be back in to\n",
      "\n",
      "-PRON- always order the radish cake calamari both be really good but -PRON- be a bit spicy\n",
      "\n",
      "and typically -PRON- will order a congee -PRON- really can not go wrong with any of -PRON- and a noodle dish\n",
      "\n",
      "what can -PRON- say\n",
      "\n",
      "congee and noodle be always good\n",
      "\n",
      "see that this be in -PRON- neighbourhood so decide to give -PRON- a try\n",
      "\n",
      "-PRON- have purchase a regular croissant and an almond_croissant\n",
      "\n",
      "the croissant be very flaky however -PRON- can not say -PRON- be too pleased with the regular croissant as -PRON- have a burn taste to -PRON- which -PRON- really do not care for\n",
      "\n",
      "the almond_croissant be really good -PRON- would go back for that one\n",
      "\n",
      "-PRON- have a beautiful display of sweet which -PRON- would like to go back to try\n",
      "\n",
      "read an article online about this be the good gluten_free shop in toronto\n",
      "\n",
      "not sure if -PRON- agree\n",
      "\n",
      "-PRON- definitely have some interesting item like vegan gluten_free bread that -PRON- have not really see anywhere_else\n",
      "\n",
      "the store be much small than -PRON- think -PRON- would be base on what -PRON- website say\n",
      "\n",
      "when -PRON- read -PRON- description of prepared food and item in store\n",
      "\n",
      "-PRON- be really excited about the variety\n",
      "\n",
      "but once -PRON- find the store which be hide if -PRON- do not pay_attention -PRON- be disappoint\n",
      "\n",
      "a lot of the other item -PRON- carry on the shelf can be find in various store around toronto at a way cheap price too\n",
      "\n",
      "if -PRON- be in the neighbourhood do go in and browse\n",
      "\n",
      "otherwise not really worth the drive up there\n",
      "\n",
      "really 3.5_star be not bad sushi\n",
      "\n",
      "decent but definitely a bit on the pricy side\n",
      "\n",
      "the roll be slightly small\n",
      "\n",
      "share two roll with the hubby and feel like -PRON- want more and usually -PRON- feel full after 2 roll\n",
      "\n",
      "-PRON- have try sushi_couture maki signature roll\n",
      "\n",
      "salmon scallop tobiko -PRON- typically have spicy mayo but -PRON- ask for no mayo since the other roll have -PRON- princess maki\n",
      "\n",
      "spicy_tuna wrap with avocado bbq eel tuna and salmon both roll be delicious just wish -PRON- be slightly big\n",
      "\n",
      "hand make noodle\n",
      "\n",
      "fast\n",
      "\n",
      "great service\n",
      "\n",
      "open kitchen\n",
      "\n",
      "need i to say more\n",
      "\n",
      "there be a line up when -PRON- get there but -PRON- move pretty fast approximately 10_min wait\n",
      "\n",
      "as soon as the server see -PRON- there wait -PRON- offer -PRON- a glass of water\n",
      "\n",
      "staff be extremely friendly and nice\n",
      "\n",
      "service be really fast\n",
      "\n",
      "even faster when -PRON- speak chinese to -PRON-\n",
      "\n",
      "-PRON- boyfriend and -PRON- have order the soup dumpling sweet and sour veggie pork noodle and a beef noodle\n",
      "\n",
      "the soup dumpling be good as -PRON- be make on the spot\n",
      "\n",
      "the veggie pork handmade noodle be really good\n",
      "\n",
      "the beef noodle not so much\n",
      "\n",
      "-PRON- be definitely know more for -PRON- handmade noodle\n",
      "\n",
      "will be return the next time -PRON- be in town to try the spicy noodle and shanghai_noodle everyone be rave_about\n",
      "\n",
      "good cheap sushi\n",
      "\n",
      "the party tray in particular be a good deal\n",
      "\n",
      "see that there be a new place in the neighborhood and decide to try -PRON- out\n",
      "\n",
      "-PRON- do not disappoint\n",
      "\n",
      "have order the buttermilk fry chicken sandwich and a classic poutine to go\n",
      "\n",
      "the buttermilk fry chicken sandwich be excellent\n",
      "\n",
      "-PRON- be very light and be not greasy at all\n",
      "\n",
      "will definitely be back to try the other item on the menu\n",
      "\n",
      "as for the poutine -PRON- be soggy and nothing_special\n",
      "\n",
      "will be back to try the onion_ring\n",
      "\n",
      "nice and light tasting cake\n",
      "\n",
      "try an original and matcha_green_tea slice\n",
      "\n",
      "similar to lady m cake\n",
      "\n",
      "will be back to check on the ice_cream\n",
      "\n",
      "-PRON- have these guy cater -PRON- wedding\n",
      "\n",
      "-PRON- end up have 6 different flavor of cake\n",
      "\n",
      "all be delicious although the carrot_cake be especially exceptional\n",
      "\n",
      "the staff be friendly and prepared\n",
      "\n",
      "all around a great experience\n",
      "\n",
      "visit from sf\n",
      "\n",
      "check yelp and find this place\n",
      "\n",
      "-PRON- be very small a convert house\n",
      "\n",
      "as -PRON- sit in -PRON- -PRON- can watch the kitchen\n",
      "\n",
      "the food be excellent\n",
      "\n",
      "-PRON- have an eggplant red_pepper omelet and peach waffle\n",
      "\n",
      "the waffle be light and fluffy with fresh whip_creme\n",
      "\n",
      "-PRON- get pastry to go which be also good in particular the chocolate croissant be unique\n",
      "\n",
      "great place for an informal breakfast\n",
      "\n",
      "-PRON- give up on this place\n",
      "\n",
      "-PRON- have get burrito and other thing from this place on several_occasion\n",
      "\n",
      "and -PRON- be always be sub_par\n",
      "\n",
      "bland food overcook sometimes burn nachos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 320, 520):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write the transformed text out to a new file, with one review per line.\n",
    "\n",
    "trigram_reviews_filepath = 'trigram_transformed_reviews_lv_rest.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_txt_filepath = 'review_text_lv_rest_subset_30.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlrognstad/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                      batch_size=10000, n_threads=4):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                              if not punct_space(token)]\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review\n",
    "                              if term not in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
